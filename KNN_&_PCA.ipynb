{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumitabh-naskar/KNN-PCA/blob/main/KNN_%26_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems?"
      ],
      "metadata": {
        "id": "CwrRAzcNcS9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-Nearest Neighbors (KNN)** is a simple, non-parametric machine learning algorithm used for both **classification** and **regression**. It's considered a \"lazy\" learning algorithm because it doesn't build a model during the training phase. Instead, it memorizes the entire dataset and performs all the computational work during prediction.\n",
        "\n",
        "***\n",
        "\n",
        "#### How KNN Works\n",
        "\n",
        "The core idea of KNN is that an object's characteristics are similar to those of its neighbors. When you want to make a prediction for a new, unknown data point, the algorithm follows these steps:\n",
        "\n",
        "1.  **Choose a number K**: This is the number of neighbors to consider.\n",
        "2.  **Calculate Distance**: It calculates the distance from the new data point to all other points in the training dataset. Common distance metrics include Euclidean distance or Manhattan distance.\n",
        "3.  **Find the K-Nearest Neighbors**: The algorithm identifies the K data points in the training set that are closest to the new point based on the calculated distances.\n",
        "4.  **Make a Prediction**:\n",
        "    * **For Classification**: It looks at the classes of the K-nearest neighbors and predicts the class that is most common among them. For example, if K=5 and three neighbors are \"Class A\" and two are \"Class B\", the new data point will be classified as \"Class A\".\n",
        "    * **For Regression**: It takes the average of the values of the K-nearest neighbors to predict a continuous value for the new data point. For example, if K=5 and the neighbor values are [10, 12, 11, 15, 12], the predicted value would be the average, which is 12."
      ],
      "metadata": {
        "id": "qFWE8DRkcP-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1: What is K-Nearest Neighbors (KNN) and how does it work in both classification and regression problems? K-Nearest Neighbors (KNN) in Classification & Regression\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris, load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "# ----------------------------\n",
        "# ðŸ”¹ KNN for Classification\n",
        "# ----------------------------\n",
        "print(\"=== KNN for Classification (Iris Dataset) ===\")\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Classifier\n",
        "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_clf = knn_clf.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Classification Accuracy:\", accuracy_score(y_test, y_pred_clf))\n",
        "\n",
        "# ----------------------------\n",
        "# >> KNN for Regression <<\n",
        "# ----------------------------\n",
        "print(\"\\n=== KNN for Regression (Diabetes Dataset) ===\")\n",
        "\n",
        "# Load dataset\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train KNN Regressor\n",
        "knn_reg = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_reg = knn_reg.predict(X_test)\n",
        "\n",
        "# Mean Squared Error\n",
        "print(\"Regression Mean Squared Error:\", mean_squared_error(y_test, y_pred_reg))\n",
        "\n",
        "# ----------------------------\n",
        "# >>Explanation:\n",
        "# - Classification â†’ Majority voting among nearest neighbors\n",
        "# - Regression â†’ Average value among nearest neighbors\n",
        "# - Scaling features is required since KNN relies on distances\n",
        "# ----------------------------\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ7m0V895TEZ",
        "outputId": "78797f57-03d0-4571-8800-5de15ae60eb2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== KNN for Classification (Iris Dataset) ===\n",
            "Classification Accuracy: 1.0\n",
            "\n",
            "=== KNN for Regression (Diabetes Dataset) ===\n",
            "Regression Mean Squared Error: 3047.449887640449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "\n",
        "Ans- Curse of Dimensionality & Its Effect on KNN\n",
        "\n",
        "What is the Curse of Dimensionality?\n",
        "\n",
        "The curse of dimensionality refers to problems that arise when the number of features (dimensions) in the dataset becomes very large.\n",
        "\n",
        "In high-dimensional space:\n",
        "\n",
        "- Data becomes sparse â†’ Points are far apart.\n",
        "\n",
        "- Distance measures lose meaning â†’ The difference between the nearest and farthest neighbor becomes very small.\n",
        "\n",
        "- More data needed â†’ The volume of space increases exponentially, so we need exponentially more data to maintain density.\n",
        "\n",
        "How it Affects KNN Performance?\n",
        "\n",
        "Since KNN relies on distance (Euclidean/Manhattan, etc.), the curse of dimensionality creates problems:\n",
        "\n",
        "- Distances become less meaningful â†’ All points seem equally far, so KNN struggles to identify true neighbors.\n",
        "\n",
        "- Overfitting risk â†’ With many irrelevant features, noise dominates the distance calculation.\n",
        "\n",
        "- High computation cost â†’ Distance calculation in high dimensions is very expensive.\n",
        "\n",
        "Ways to Reduce the Effect:\n",
        "\n",
        "- Feature Selection â†’ Keep only relevant features.\n",
        "\n",
        "- Dimensionality Reduction â†’ Use PCA, t-SNE, or autoencoders.\n",
        "\n",
        "- Scaling/Normalization â†’ Helps but does not fully solve the issue."
      ],
      "metadata": {
        "id": "j05-fFs3501a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3: What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a dimensionality reduction technique used to simplify a dataset while retaining its most important information. It transforms a set of correlated variables into a smaller set of uncorrelated variables called **principal components**. The first principal component accounts for the largest possible variance in the data, the second component accounts for the next largest variance orthogonal to the first, and so on. Essentially, PCA creates new, synthesized features from the original ones. .\n",
        "\n",
        "***\n",
        "\n",
        "#### PCA vs. Feature Selection\n",
        "\n",
        "While both PCA and feature selection are techniques for dimensionality reduction, they achieve it in fundamentally different ways:\n",
        "\n",
        "* **PCA (Feature Extraction)**: PCA doesn't remove features; it transforms the existing ones into a new set of components. It creates a new, smaller set of features that are linear combinations of the original features. This can be very useful for reducing the complexity of a dataset and combating the \"curse of dimensionality.\" However, the new components can be difficult to interpret, as they don't directly correspond to any of the original variables.\n",
        "\n",
        "* **Feature Selection**: This is a process that chooses a **subset of the most relevant features** from the original dataset and discards the rest. The selected features are the actual, original variables. Feature selection methods can be simple (like removing features with low variance) or more complex (like using a model to rank feature importance). The primary advantage of feature selection is that the final model is more interpretable because it uses the original, understandable features. However, it may discard valuable information contained in the discarded features.\n",
        "\n"
      ],
      "metadata": {
        "id": "X_o1dbke68_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4: What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "\n",
        "Ans- Eigenvalues & Eigenvectors in PCA\n",
        "\n",
        "What are Eigenvalues & Eigenvectors?\n",
        "\n",
        "- Eigenvectors: Directions along which data varies the most (principal components).\n",
        "\n",
        "- Eigenvalues: The amount of variance captured along each eigenvector (importance/weight of each component).\n",
        "\n",
        " In PCA:\n",
        "\n",
        "- Eigenvectors = new feature axes (principal components).\n",
        "\n",
        "- Eigenvalues = how much variance (information) each component explains.\n",
        "\n",
        "Why are They Important in PCA?\n",
        "\n",
        "1. Eigenvectors determine the orientation of new axes (principal components).\n",
        "\n",
        "2. Eigenvalues tell us how much information (variance) is retained by each component.\n",
        "\n",
        "3. We use the largest eigenvalues â†’ corresponding eigenvectors form the reduced feature space.\n",
        "\n",
        "4. Helps in deciding how many components to keep (e.g., keep components that explain 95% variance)."
      ],
      "metadata": {
        "id": "9r1OMaR2_vZU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5: How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "\n",
        "Ans- How KNN and PCA Complement Each Other\n",
        "\n",
        "KNN Recap\n",
        "\n",
        "- KNN is a distance-based algorithm.\n",
        "\n",
        "- Performance depends heavily on feature space and distances.\n",
        "\n",
        "- Struggles in high-dimensional data (curse of dimensionality).\n",
        "\n",
        "PCA Recap\n",
        "\n",
        "- PCA reduces dimensionality by creating new features (principal components).\n",
        "\n",
        "- Removes noise and correlations between features.\n",
        "\n",
        "- Retains maximum variance in fewer dimensions.\n",
        "\n",
        "How They Work Together\n",
        "\n",
        "- PCA before KNN â†’ PCA reduces dimensionality, keeping only the most informative components.\n",
        "\n",
        "- This makes distances more meaningful for KNN (less noise, less redundancy).\n",
        "\n",
        "- PCA also reduces computation cost â†’ KNN is faster with fewer features.\n",
        "\n",
        "- PCA helps avoid overfitting in KNN by removing irrelevant/weak features."
      ],
      "metadata": {
        "id": "z_q7wOaNBysc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases."
      ],
      "metadata": {
        "id": "t6ImItz6dLId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "# The Wine dataset is a classic classification problem with 13 features.\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
        "\n",
        "# 2. Case 1: Train KNN without feature scaling\n",
        "# We train the KNN classifier directly on the raw data.\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(\"Accuracy without feature scaling:\")\n",
        "print(f\"{accuracy_unscaled:.4f}\")\n",
        "\n",
        "# 3. Case 2: Train KNN with feature scaling\n",
        "# We use StandardScaler to normalize the features. This is a crucial step\n",
        "# for distance-based algorithms like KNN.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN classifier on the scaled data.\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"\\nAccuracy with feature scaling:\")\n",
        "print(f\"{accuracy_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jwzs1RR5Cf9T",
        "outputId": "cd21bfe1-7fac-4e6c-cdef-2860e9d71f1e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling:\n",
            "0.7037\n",
            "\n",
            "Accuracy with feature scaling:\n",
            "0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component."
      ],
      "metadata": {
        "id": "4UJrwyfdb2a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "# The Wine dataset is a classic classification problem with 13 features.\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Case 1: Train KNN without feature scaling\n",
        "# We train the KNN classifier directly on the raw data.\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(\"Accuracy without feature scaling:\")\n",
        "print(f\"{accuracy_unscaled:.4f}\")\n",
        "\n",
        "# 3. Case 2: Train KNN with feature scaling\n",
        "# We use StandardScaler to normalize the features. This is a crucial step\n",
        "# for distance-based algorithms like KNN.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN classifier on the scaled data.\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"\\nAccuracy with feature scaling:\")\n",
        "print(f\"{accuracy_scaled:.4f}\")\n",
        "\n",
        "# 4. Train a PCA model and print explained variance ratio\n",
        "# We train a PCA model on the scaled training data.\n",
        "# The `explained_variance_ratio_` attribute shows the proportion of\n",
        "# variance in the data that is captured by each principal component.\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(\"\\nExplained Variance Ratio of Principal Components:\")\n",
        "print(explained_variance_ratio)\n",
        "\n",
        "# Sum of the first two components to show cumulative variance\n",
        "cumulative_variance = np.sum(explained_variance_ratio[:2])\n",
        "print(f\"\\nCumulative explained variance of the first two components: {cumulative_variance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rRLifSJCtcp",
        "outputId": "4b039f8a-dc44-4f4f-e5d2-246609134d85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling:\n",
            "0.7407\n",
            "\n",
            "Accuracy with feature scaling:\n",
            "0.9630\n",
            "\n",
            "Explained Variance Ratio of Principal Components:\n",
            "[0.36196226 0.18763862 0.11656548 0.07578973 0.07043753 0.04552517\n",
            " 0.03584257 0.02646315 0.02174942 0.01958347 0.01762321 0.01323825\n",
            " 0.00758114]\n",
            "\n",
            "Cumulative explained variance of the first two components: 0.5496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset."
      ],
      "metadata": {
        "id": "K7rowTdpbs12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "# The Wine dataset is a classic classification problem with 13 features.\n",
        "wine_data = load_wine()\n",
        "X = wine_data.data\n",
        "y = wine_data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Case 1: Train KNN without feature scaling\n",
        "# We train the KNN classifier directly on the raw data.\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "y_pred_unscaled = knn_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(\"Accuracy without feature scaling:\")\n",
        "print(f\"{accuracy_unscaled:.4f}\")\n",
        "\n",
        "# 3. Case 2: Train KNN with feature scaling\n",
        "# We use StandardScaler to normalize the features. This is a crucial step\n",
        "# for distance-based algorithms like KNN.\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the KNN classifier on the scaled data.\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = knn_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(\"\\nAccuracy with feature scaling:\")\n",
        "print(f\"{accuracy_scaled:.4f}\")\n",
        "\n",
        "# 4. Train a PCA model and print explained variance ratio\n",
        "# We train a PCA model on the scaled training data.\n",
        "# The `explained_variance_ratio_` attribute shows the proportion of\n",
        "# variance in the data that is captured by each principal component.\n",
        "pca = PCA()\n",
        "pca.fit(X_train_scaled)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(\"\\nExplained Variance Ratio of Principal Components:\")\n",
        "print(explained_variance_ratio)\n",
        "\n",
        "# Sum of the first two components to show cumulative variance\n",
        "cumulative_variance = np.sum(explained_variance_ratio[:2])\n",
        "print(f\"\\nCumulative explained variance of the first two components: {cumulative_variance:.4f}\")\n",
        "\n",
        "# 5. Case 3: Train KNN on PCA-transformed data (top 2 components)\n",
        "# We use PCA to reduce the dimensionality to the top 2 principal components.\n",
        "# This helps to remove noise and can improve performance for KNN.\n",
        "pca_2_components = PCA(n_components=2)\n",
        "X_train_pca = pca_2_components.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca_2_components.transform(X_test_scaled)\n",
        "\n",
        "# Train a new KNN classifier on the PCA-transformed data\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "print(\"\\nAccuracy with PCA (top 2 components):\")\n",
        "print(f\"{accuracy_pca:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bn_62fO5DGlz",
        "outputId": "aca15b87-0424-4036-adbd-d36496d06996"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without feature scaling:\n",
            "0.7407\n",
            "\n",
            "Accuracy with feature scaling:\n",
            "0.9630\n",
            "\n",
            "Explained Variance Ratio of Principal Components:\n",
            "[0.36196226 0.18763862 0.11656548 0.07578973 0.07043753 0.04552517\n",
            " 0.03584257 0.02646315 0.02174942 0.01958347 0.01762321 0.01323825\n",
            " 0.00758114]\n",
            "\n",
            "Cumulative explained variance of the first two components: 0.5496\n",
            "\n",
            "Accuracy with PCA (top 2 components):\n",
            "0.9815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9: Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for KNN)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --------------------------\n",
        "# KNN with Euclidean Distance (default)\n",
        "# --------------------------\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
        "knn_euclidean.fit(X_train_scaled, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test_scaled)\n",
        "acc_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "# --------------------------\n",
        "# KNN with Manhattan Distance\n",
        "# --------------------------\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='manhattan')\n",
        "knn_manhattan.fit(X_train_scaled, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test_scaled)\n",
        "acc_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "# --------------------------\n",
        "# Results\n",
        "# --------------------------\n",
        "print(\"Accuracy with Euclidean Distance:\", acc_euclidean)\n",
        "print(\"Accuracy with Manhattan Distance:\", acc_manhattan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlRngUxCDaNz",
        "outputId": "803ecb19-9a6f-449d-cee6-aba83648a965"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with Euclidean Distance: 0.9444444444444444\n",
            "Accuracy with Manhattan Distance: 0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. You are working with a high-dimensional gene expression dataset to classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models overfit.\n",
        "Explain how you would:\n",
        "- Use PCA to reduce dimensionality\n",
        "- Decide how many components to keep\n",
        "- Use KNN for classification post-dimensionality reduction\n",
        "- Evaluate the model\n",
        "- Justify this pipeline to your stakeholders as a robust solution for real-world biomedical data"
      ],
      "metadata": {
        "id": "8LRnsl8Ra26O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline for High-Dimensional Gene Expression Classification\n",
        "\n",
        "#### 1. **Use PCA to Reduce Dimensionality**\n",
        "\n",
        "Gene expression data often has thousands of features (genes) and very few samples. PCA helps by transforming the data into a set of orthogonal components that capture the most variance.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Standardize the data\n",
        "X_scaled = StandardScaler().fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Decide How Many Components to Keep**\n",
        "\n",
        "We would want to retain enough components to preserve most of the variance while reducing noise and overfitting risk.\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Choose number of components that explain 95% variance\n",
        "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95) + 1\n",
        "\n",
        "# Re-apply PCA with selected components\n",
        "pca = PCA(n_components=n_components)\n",
        "X_reduced = pca.fit_transform(X_scaled)\n",
        "```\n",
        "---\n",
        "\n",
        "#### 3. **Use KNN for Classification Post-PCA**\n",
        "\n",
        "KNN is simple, non-parametric, and works well when the feature space is clean and reduced.\n",
        "\n",
        "```python\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Tune the number of neighbors\n",
        "param_grid = {'n_neighbors': list(range(1, 21))}\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_knn = grid_search.best_estimator_\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Evaluate the Model**\n",
        "\n",
        "Use stratified cross-validation and multiple metrics to ensure robustness.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred = best_knn.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "```\n",
        "---\n",
        "\n",
        "#### 5. **Justify the Pipeline to Stakeholders**\n",
        "\n",
        "Hereâ€™s how it has happened:\n",
        "\n",
        "- **Dimensionality Reduction:** PCA combats overfitting and reveals latent biological signals.\n",
        "- **Interpretability:** PCA components can be traced back to gene contributions, aiding biological insight.\n",
        "- **Simplicity & Transparency:** KNN is intuitive and easy to explain, especially in clinical settings.\n",
        "- **Validation:** Cross-validation and metric-based evaluation ensure the model generalizes well.\n",
        "- **Scalability:** The pipeline is modular and can be extended to other classifiers or integrated with biological priors."
      ],
      "metadata": {
        "id": "41RvvPv3bNj9"
      }
    }
  ]
}